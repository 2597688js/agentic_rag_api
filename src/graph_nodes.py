from langgraph.graph import MessagesState
from langchain.chat_models import init_chat_model
from pydantic import BaseModel, Field
from typing import Literal
from langgraph.graph import MessagesState
from src.pydantic_models import GradeDocuments
import logging

# Set up logging
logger = logging.getLogger(__name__)

# --- get the config ---
try:
    from src.config import ConfigManager
    config = ConfigManager()._config
except Exception as e:
    print(f"Failed to load config in graph_nodes.py: {e}, using fallback")
    config = {
        'model_config': {
            'response_model': 'gpt-3.5-turbo',
            'grader_model': 'gpt-3.5-turbo',
            'temperature': 0.7
        },
        'prompts': {
            'GRADE_PROMPT': 'You are a document relevance grader. Your task is to determine if the retrieved documents are relevant to the user\'s question.\n\nQuestion: {question}\nRetrieved Context: {context}\n\nGrade the relevance using a binary score:\n- "yes" if the documents are relevant and can help answer the question\n- "no" if the documents are not relevant or insufficient\n\nBinary Score:',
            'REWRITE_PROMPT': 'You are a question rewriter. The user\'s question was not answered well by the retrieved documents.\n\nOriginal Question: {question}\n\nPlease rewrite the question to be more specific, clear, or focused.\n\nRewritten Question:',
            'GENERATE_PROMPT': 'You are an AI assistant that answers questions based on retrieved document content.\n\nQuestion: {question}\nRetrieved Context: {context}\n\nPlease provide a comprehensive answer based on the context provided. If the context doesn\'t contain enough information to answer the question completely, acknowledge what you can answer and what information is missing.\n\nAnswer:'
        }
    }

# --- initialize the models ---
response_model = init_chat_model(
    config['model_config']['response_model'], 
    temperature=config['model_config']['temperature']
    )

grader_model = init_chat_model(
    config['model_config']['grader_model'], 
    temperature=config['model_config']['temperature']
    )


class State(MessagesState):
    documents: list[str]
    
def generate_query_or_respond(retriever_tool, state: MessagesState):
    """Call the model to generate a response based on the current state. Given
    the question, it will decide to retrieve using the retriever tool, or simply respond to the user.
    """
    # Log the messages being sent to the model
    # logger.info("=== GENERATE_QUERY_OR_RESPOND INPUT ===")
    # for msg in state["messages"]:
    #     logger.info(f"Message: {msg.content}")
    # logger.info("=== END GENERATE_QUERY_OR_RESPOND INPUT ===")
    
    response = (
        response_model
        .bind_tools([retriever_tool]).invoke(state["messages"])
    )
    
    # Log the response
    # logger.info("=== GENERATE_QUERY_OR_RESPOND OUTPUT ===")
    # logger.info(f"Response: {response.content}")
    # if hasattr(response, 'tool_calls') and response.tool_calls:
    #     logger.info(f"Tool calls: {[tc.get('name') for tc in response.tool_calls]}")
    # else:
    #     logger.info("No tool calls - responding directly")
    # logger.info("=== END GENERATE_QUERY_OR_RESPOND OUTPUT ===")
    
    return {"messages": [response]}


def grade_documents(
    state: MessagesState,
) -> Literal["generate_answer", "rewrite_question"]:
    """Determine whether the retrieved documents are relevant to the question."""
    question = state["messages"][0].content
    context = state["messages"][-1].content

    # Count how many times we've been through this loop
    # Look for actual rewrite_question node executions in the message history
    rewrite_count = 0
    for msg in state["messages"]:
        if hasattr(msg, 'content') and msg.content:
            # Check if this message was generated by the rewrite_question node
            # We can identify this by looking for patterns that indicate a rewritten question
            if any(keyword in msg.content.lower() for keyword in ["reformulated", "rewritten", "rephrased", "clarified"]):
                rewrite_count += 1
    
    # If we've rewritten the question too many times, just generate an answer
    if rewrite_count >= 3:  # Increased threshold for safety
        logger.warning(f"Question rewritten {rewrite_count} times, forcing answer generation")
        return "generate_answer"

    prompt = config['prompts']['GRADE_PROMPT'].format(question=question, context=context)
    
    # Log the prompt being sent to the grader
    # logger.info("=== GRADE_DOCUMENTS PROMPT ===")
    # logger.info(prompt)
    # logger.info("=== END GRADE_DOCUMENTS PROMPT ===")
    
    response = (
        grader_model
        .with_structured_output(GradeDocuments).invoke(
            [{"role": "user", "content": prompt}]
        )
    )
    score = response.binary_score
    
    # logger.info(f"GRADING RESULT: {score}")
    
    if score == "yes":
        return "generate_answer"
    else:
        return "rewrite_question"

def rewrite_question(state: MessagesState):
    """Rewrite the original user question."""
    messages = state["messages"]
    question = messages[0].content
    prompt = config['prompts']['REWRITE_PROMPT'].format(question=question)
    
    # Log the prompt being sent to the rewriter
    # logger.info("=== REWRITE_QUESTION PROMPT ===")
    # logger.info(prompt)
    # logger.info("=== END REWRITE_QUESTION PROMPT ===")
    
    response = response_model.invoke([{"role": "user", "content": prompt}])
    return {"messages": [{"role": "user", "content": response.content}]}


def generate_answer(state: MessagesState):
    """Generate an answer."""
    question = state["messages"][0].content
    context = state["messages"][-1].content
    
    # Preprocess the question to handle "provided document" references
    processed_question = question
    if "provided document" in question.lower() or "the document" in question.lower():
        processed_question = question.replace("provided document", "retrieved document content").replace("the document", "the retrieved document content")
    
    prompt = config['prompts']['GENERATE_PROMPT'].format(question=processed_question, context=context)
    
    # Log the prompt being sent to the generator
    # logger.info("=== GENERATE_ANSWER PROMPT ===")
    # logger.info(f"Original Question: {question}")
    # logger.info(f"Processed Question: {processed_question}")
    # logger.info("FULL PROMPT:")
    # logger.info(prompt)
    # logger.info("=== END GENERATE_ANSWER PROMPT ===")
    
    response = response_model.invoke([{"role": "user", "content": prompt}])
    return {"messages": [response]}
